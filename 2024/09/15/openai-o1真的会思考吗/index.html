<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>OpenAI-o1真的会思考吗 | LZ Blog</title>
  <link rel="stylesheet" href="/css/fonts/Chinese-normal-normal.min.css">
  <link rel="stylesheet" href="/css/fonts/ChineseMono-normal-normal.min.css">
  <link rel="stylesheet" href="/css/fonts/Chinese-italic-normal.min.css">
  <link rel="stylesheet" href="/css/fonts/Chinese-normal-bold.min.css">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <meta name="description" content="在OpenAI的眼中，实现通用人工智能大致要经历5个阶段：对话(conversation)，推理(reasoning)，代理(agent)，创新(innovation)，组织(organization)。OpenAI的o1模型已经达到了推理这一阶段，相信第三阶段也指日可待。在它到达第四阶段—创新—之前，个人认为还需要达到”直觉“这个阶段。
OpenAI在上周推出了最新的模型o1-preview。名字的含义是Orion（猎户座）一代。有人说计数器重置为1，因此很可能不会有GPT-5，o1将开启一个全新的时代。很多人对o1模型能力进行了解读，也有大量的新奇的应用如雨后春笋一般涌现出来，总得来说o1是思维链(Chain of Thought, COT)的集大成者，它很可能推动LLM新一轮的范式改变：侧重点从训练到推理转移，然后以推理时间的增长来继续scaling law。但是它真的会“思考”吗？">
  
  
    <link rel="alternate" href="/atom.xml" title="LZ Blog" type="application/atom+xml">
  
  
  <link rel="stylesheet" href="/css/style.css">
  
    <link rel="stylesheet" href="/fancybox/jquery.fancybox-1.3.4.css">
  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div id="nav-outer">
  <nav id="main-nav" class="outer">
    <a id="main-nav-toggle" class="nav-icon"></a>
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" target="_blank" rel="noopener" href="https://tinymind.me/zleung9/thoughts">Thoughts</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
    <div class="main-nav-space-between"></div>
    
      <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
    
  </nav>
</div>
<!-- Debug info -->

  <div id="header-title">
    <h1 id="logo-wrap">
      <a href="/" id="logo">LZ Blog</a>
    </h1>
    
      <h2 id="subtitle-wrap">
        <a href="/" id="subtitle">我们向着太阳奔跑，试图甩掉身后的影子</a>
      </h2>
    
  </div>


      <div id="content" class="outer">
        <section id="main"><article id="post-openai-o1真的会思考吗" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/09/15/openai-o1%E7%9C%9F%E7%9A%84%E4%BC%9A%E6%80%9D%E8%80%83%E5%90%97/" class="article-date">
  <time class="dt-published" datetime="2024-09-16T05:41:01.000Z" itemprop="datePublished">2024-09-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Article/">Article</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      OpenAI-o1真的会思考吗
    </h1>
  

      </header>
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><em>在OpenAI的眼中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>实现通用人工智能大致要经历5个阶段<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>对话(conversation)<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>推理(reasoning)<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>代理(agent)<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>创新(innovation)<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>组织(organization)<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>OpenAI的o1模型已经达到了推理这一阶段<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>相信第三阶段也指日可待<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在它到达第四阶段—创新—之前<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>个人认为还需要达到<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>直觉<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>这个阶段<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></em></p>
<p>OpenAI在上周推出了最新的模型o1-preview<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>名字的含义是Orion<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>猎户座<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>一代<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>有人说计数器重置为1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因此很可能不会有GPT-5<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>o1将开启一个全新的时代<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br>很多人对o1模型能力进行了解读<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>也有大量的新奇的应用如雨后春笋一般涌现出来<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>总得来说o1是思维链(Chain of Thought, COT)的集大成者<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它很可能推动LLM新一轮的范式改变<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>侧重点从训练到推理转移<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后以推理时间的增长来继续scaling law<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但是它真的会<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>思考<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>吗<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span></p>
<span id="more"></span>
<h1 id="o1模型本质上就是把思维链内化"><a href="#o1模型本质上就是把思维链内化" class="headerlink" title="o1模型本质上就是把思维链内化"></a>o1模型本质上就是把思维链内化</h1><p>o1本质上就是把思维链<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>COT<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>内化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>之前需要依赖人工写COT来指导LLM接近正确答案<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>现在OpenAI的把COT隐匿地训练到了o1的内部<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>就像AlphaGo下棋<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>形成了巨大的树形搜索空间<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>相当于一条条COT构成的空间<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这里COT的具体步骤的组会空间是巨大的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>人写的COT未必是最优的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>问题越复杂<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这个树的搜索空间就越大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>搜索复杂度就越高<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>找到正确答案涉及到的COT步聚越多<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br>这一突破意味着复杂的提示词工程(Prompt Engineering)可能会逐渐淡出舞台<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>未来,用户无需再构造繁琐的提示词,AI系统将能够自主生成最适合的思维过程,这无疑是向着更高度智能化迈进的一大步<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h1 id="从训练到推理的转变"><a href="#从训练到推理的转变" class="headerlink" title="从训练到推理的转变"></a><strong>从训练到推理的转变</strong></h1><p>简单的来说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>OpenAI o1 系列模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在复杂推理上的性能提升模式<strong>与传统 LLM 预训练式的性能提升不同</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>主要<strong>通过强化学习的方式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>让模型不断完善思考过程</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>包括对不同策略进行尝试<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>认识到错误等<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这个过程是推理阶段完成的<br>因此<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们应该有两种方式来增加能力<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>在训练期间以及在推理期间<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这实际上对Nvidia的竞争对手如Groq和SambaNova Systems来说是一个利好信号<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为他们有更多机会在推理计算方面竞争<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而在训练计算方面几乎没有机会竞争<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h1 id="o1会思考吗"><a href="#o1会思考吗" class="headerlink" title="o1会思考吗"></a><strong>o1会思考吗</strong></h1><p>人类的物理数学大师一定程度上是不是因为他们计算能力强<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>解决具体问题的逻辑能力强<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而是他们的强大的<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>直觉<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>物理学中有很多奠基性的理论<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>都是物理学家先有了答案<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后再回头去证明和推理心中的那个答案<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>他们的答案是如何来的呢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span>这就是大师们的<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>直觉<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<blockquote>
<p>A mathematician is a person who can find analogies between theorems; a better mathematician is one who can see analogies between proofs and the best mathematician can notice analogies between theories. One can imagine that the ultimate mathematician is one who can see analogies between analogies.<br>-—- Stefan Banach</p>
</blockquote>
<p>LLM的预训练所得的是对一种简单规则的<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>直觉<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>以前我们通过大量的数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>让LLM抽象出了数据背后最一般的映射<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>也就是规则<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>比如语言里的语法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一些简单的数学规则<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>基于统计意义上的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但是很多复杂问题是多个简单规则的嵌套<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这时候LLM就力不从心了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为它无法通过<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>死记硬背<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>的方法来学会这些规则组合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>现在我们有o1模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以借助COT来解决这些问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这时COT有点像人类的逻辑思考<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>或者Reasoning<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>近一步想<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果通过大量的COT作为数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>是不是可以训练出对复杂问题的<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>直觉<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>呢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span>这个时候的LLM是不是就更像一个科学家那样<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>思考了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span></p>

      
    </div>
    <footer class="article-footer">
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag">人工智能</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2024/09/22/%E6%88%91%E6%89%80%E7%90%86%E8%A7%A3%E7%9A%84%E5%88%9B%E5%A7%8B%E4%BA%BA%E6%A8%A1%E5%BC%8F/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          我所理解的创始人模式
        
      </div>
    </a>
  
  
    <a href="/2024/08/31/limu-languagemodel/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">
        
          李沐：语言模型的现状与未来
        
      </div>
    </a>
  
</nav>

  
</article>


</section>
        
      </div>
      <footer id="footer">
  
    <aside id="sidebar" class="outer">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 20px;">人工智能</a> <a href="/tags/%E5%88%9B%E6%8A%95/" style="font-size: 13.33px;">创投</a> <a href="/tags/%E5%8E%86%E5%8F%B2/" style="font-size: 16.67px;">历史</a> <a href="/tags/%E5%95%86%E4%B8%9A/" style="font-size: 10px;">商业</a> <a href="/tags/%E5%A4%A7%E8%84%91/" style="font-size: 10px;">大脑</a> <a href="/tags/%E5%B0%8F%E8%AF%B4/" style="font-size: 10px;">小说</a> <a href="/tags/%E7%A7%91%E5%B9%BB/" style="font-size: 13.33px;">科幻</a> <a href="/tags/%E7%A7%91%E6%8A%80/" style="font-size: 20px;">科技</a>
    </div>
  </div>

  
</aside>
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
        <p>&copy; 2025 Zhu Liang</p>
      
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a target="_blank" rel="noopener" href="https://tinymind.me/zleung9/thoughts" class="mobile-nav-link">Thoughts</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav>
    

<script src="/js/clipboard.min.js"></script>
<script src="/js/jquery-1.4.3.min.js"></script>

<script src="/fancybox/jquery.fancybox-1.3.4.pack.js"></script>


<script src="/js/script.js"></script>






<script>
  MathJax = {
    options: {
      enableMenu: false
    },
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
    }
  };
</script>
<!-- <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    CommonHTML: {
      linebreaks: false
    }
  });
  </script> -->
<script type="text/javascript" id="MathJax-script" async
  src="/mathjax/tex-chtml.js">
</script>
<!-- <script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML">
</script> -->

  </div>
</body>
</html>